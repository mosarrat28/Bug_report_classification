{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##mozilla"
      ],
      "metadata": {
        "id": "SRzAh2OQCvtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-d4vxH1euxv",
        "outputId": "a88652b2-f95d-4c59-f5c6-b6fb23c909ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 10 21:38:06 2024\n",
            "Preprocessing training data...\n",
            "Number of valid training documents: 7825\n",
            "Preprocessing test data...\n",
            "Number of valid test documents: 1957\n",
            "Building vocabulary...\n",
            "Vocabulary size (DM): 27470\n",
            "Vocabulary size (DBOW): 27469\n",
            "Training models...\n",
            "Generating training data...\n",
            "Generating testing data...\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.7379\n",
            "Precision | Recall | F-Score\n",
            "(0.7195068326466643, 0.7378640776699029, 0.7278383787618734, None)\n",
            "Tue Dec 10 21:42:54 2024\n",
            "TOTAL RUNTIME:  287.48146080970764 s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import multiprocessing\n",
        "\n",
        "# Helper functions for data loading and preprocessing\n",
        "def load_data(dataset, train=True, percent=0.8):\n",
        "    '''Reads in and formats data from the dataset with train/test split.'''\n",
        "    df = pd.read_csv(dataset.path, sep=',', encoding='ISO-8859-1')\n",
        "    df = df.dropna(subset=[\"Severity\", \"Description\"])  # Drop rows with missing Severity or Description\n",
        "    df['Severity'] = df['Severity'].astype(int)  # Convert Severity to int\n",
        "    df['Description'] = df['Description'].astype(str).str.strip()  # Ensure Description is a string\n",
        "\n",
        "    # Filter out invalid rows\n",
        "    df = df[df['Description'] != '']\n",
        "\n",
        "    raw_data = df[['Description', 'Severity']].to_numpy()  # Use only Description and Severity columns\n",
        "\n",
        "    # Split the dataset into train and test based on the percent\n",
        "    train_data, test_data = train_test_split(raw_data, test_size=(1 - percent), random_state=42)\n",
        "    return train_data if train else test_data\n",
        "\n",
        "def preprocess(train_data, test_data):\n",
        "    '''Generate paragraph vectors using Doc2Vec.'''\n",
        "    print(\"Preprocessing training data...\")\n",
        "    train_corpus = list(_read_corpus(train_data))\n",
        "    print(f\"Number of valid training documents: {len(train_corpus)}\")\n",
        "\n",
        "    print(\"Preprocessing test data...\")\n",
        "    test_corpus = list(_read_corpus(test_data, tokens_only=True))\n",
        "    print(f\"Number of valid test documents: {len(test_corpus)}\")\n",
        "\n",
        "    if len(train_corpus) == 0 or len(test_corpus) == 0:\n",
        "        raise ValueError(\"No valid data found after preprocessing. Check the 'Description' column.\")\n",
        "\n",
        "    cores = max(1, multiprocessing.cpu_count() // 2)  # Use half the cores\n",
        "\n",
        "    # Initialize Doc2Vec models\n",
        "    model_DM = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=1, epochs=100, workers=cores, dm=1, dm_concat=1)\n",
        "    model_DBOW = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=1, epochs=100, workers=cores, dm=0)\n",
        "\n",
        "    # Build vocabulary\n",
        "    print(\"Building vocabulary...\")\n",
        "    model_DM.build_vocab(train_corpus)\n",
        "    model_DBOW.build_vocab(train_corpus)\n",
        "\n",
        "    print(f\"Vocabulary size (DM): {len(model_DM.wv.key_to_index)}\")\n",
        "    print(f\"Vocabulary size (DBOW): {len(model_DBOW.wv.key_to_index)}\")\n",
        "\n",
        "    if len(model_DM.wv.key_to_index) == 0 or len(model_DBOW.wv.key_to_index) == 0:\n",
        "        raise ValueError(\"Vocabulary is empty. Check your input data or preprocessing.\")\n",
        "\n",
        "    # Train Doc2Vec models\n",
        "    print(\"Training models...\")\n",
        "    model_DM.train(train_corpus, total_examples=model_DM.corpus_count, epochs=model_DM.epochs)\n",
        "    model_DBOW.train(train_corpus, total_examples=model_DBOW.corpus_count, epochs=model_DBOW.epochs)\n",
        "\n",
        "    # Generate training data\n",
        "    print(\"Generating training data...\")\n",
        "    X_train = [(list(model_DM.dv[i]) + list(model_DBOW.dv[i])) for i in range(len(train_corpus))]\n",
        "    Y_train = [doc[1] for doc in train_data]\n",
        "\n",
        "    print(\"Generating testing data...\")\n",
        "    X_test = [(list(model_DM.infer_vector(test_corpus[i])) + list(model_DBOW.infer_vector(test_corpus[i]))) for i in range(len(test_corpus))]\n",
        "    Y_test = [doc[1] for doc in test_data]\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "def _read_corpus(data, tokens_only=False):\n",
        "    '''Helper function to prepare data for Doc2Vec.'''\n",
        "    for i, line in enumerate(data):\n",
        "        description = str(line[0]).strip()  # Ensure the description is a string and trimmed\n",
        "        if not description:\n",
        "            continue\n",
        "        tokens = gensim.utils.simple_preprocess(description)\n",
        "        if tokens_only:\n",
        "            yield tokens\n",
        "        else:\n",
        "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "# Classifier\n",
        "class ASP():\n",
        "    def __init__(self, X_train, Y_train, X_test, Y_test):\n",
        "        self.X_train, self.Y_train = X_train, Y_train\n",
        "        self.X_test, self.Y_test = X_test, Y_test\n",
        "        self.classifier = MLPClassifier(alpha=0.7, max_iter=10000)\n",
        "\n",
        "    def fit(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n",
        "\n",
        "    def predict(self):\n",
        "        prediction = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, prediction)\n",
        "        prf1 = precision_recall_fscore_support(y_true=self.Y_test, y_pred=prediction, average='weighted')\n",
        "\n",
        "        print('Evaluation Metrics:')\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print('Precision | Recall | F-Score')\n",
        "        pprint(prf1)\n",
        "\n",
        "# Dataset class\n",
        "class Dataset():\n",
        "    def __init__(self, path, project_id):\n",
        "        self.path = path\n",
        "        self.project_id = project_id\n",
        "\n",
        "# Experiment class\n",
        "class Experiment():\n",
        "    def __init__(self, train_data, test_data):\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def run(self):\n",
        "        X_train, Y_train, X_test, Y_test = preprocess(self.train_data, self.test_data)\n",
        "        classifier = ASP(X_train, Y_train, X_test, Y_test)\n",
        "        classifier.fit()\n",
        "        classifier.predict()\n",
        "\n",
        "# Script to run experiment with 80/20 train-test split\n",
        "a = Dataset('/content/mozilla_bug_report_data.csv', project_id=1)  # Replace with your dataset path\n",
        "train_data = load_data(a, train=True, percent=0.8)\n",
        "test_data = load_data(a, train=False, percent=0.8)\n",
        "\n",
        "print(time.ctime(time.time()))\n",
        "start = time.time()\n",
        "\n",
        "experiment = Experiment(train_data, test_data)\n",
        "experiment.run()\n",
        "\n",
        "print(time.ctime(time.time()))\n",
        "print('TOTAL RUNTIME: ', time.time() - start, 's')\n",
        "print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import multiprocessing\n",
        "\n",
        "# Helper functions for data loading and preprocessing\n",
        "def load_data(dataset, train=True, percent=0.8):\n",
        "    '''Reads in and formats data from the dataset with train/test split.'''\n",
        "    df = pd.read_csv(dataset.path, sep=',', encoding='ISO-8859-1')\n",
        "    df = df.dropna(subset=[\"Severity\", \"short_description\"])  # Drop rows with missing Severity or Description\n",
        "    df['Severity'] = df['Severity'].astype(int)  # Convert Severity to int\n",
        "    df['short_description'] = df['short_description'].astype(str).str.strip()  # Ensure Description is a string\n",
        "\n",
        "    # Filter out invalid rows\n",
        "    df = df[df['short_description'] != '']\n",
        "\n",
        "    raw_data = df[['short_description', 'Severity']].to_numpy()  # Use only Description and Severity columns\n",
        "\n",
        "    # Split the dataset into train and test based on the percent\n",
        "    train_data, test_data = train_test_split(raw_data, test_size=(1 - percent), random_state=42)\n",
        "    return train_data if train else test_data\n",
        "\n",
        "def preprocess(train_data, test_data):\n",
        "    '''Generate paragraph vectors using Doc2Vec.'''\n",
        "    print(\"Preprocessing training data...\")\n",
        "    train_corpus = list(_read_corpus(train_data))\n",
        "    print(f\"Number of valid training documents: {len(train_corpus)}\")\n",
        "\n",
        "    print(\"Preprocessing test data...\")\n",
        "    test_corpus = list(_read_corpus(test_data, tokens_only=True))\n",
        "    print(f\"Number of valid test documents: {len(test_corpus)}\")\n",
        "\n",
        "    if len(train_corpus) == 0 or len(test_corpus) == 0:\n",
        "        raise ValueError(\"No valid data found after preprocessing. Check the 'Description' column.\")\n",
        "\n",
        "    cores = max(1, multiprocessing.cpu_count() // 2)  # Use half the cores\n",
        "\n",
        "    # Initialize Doc2Vec models\n",
        "    model_DM = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=1, epochs=100, workers=cores, dm=1, dm_concat=1)\n",
        "    model_DBOW = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=1, epochs=100, workers=cores, dm=0)\n",
        "\n",
        "    # Build vocabulary\n",
        "    print(\"Building vocabulary...\")\n",
        "    model_DM.build_vocab(train_corpus)\n",
        "    model_DBOW.build_vocab(train_corpus)\n",
        "\n",
        "    print(f\"Vocabulary size (DM): {len(model_DM.wv.key_to_index)}\")\n",
        "    print(f\"Vocabulary size (DBOW): {len(model_DBOW.wv.key_to_index)}\")\n",
        "\n",
        "    if len(model_DM.wv.key_to_index) == 0 or len(model_DBOW.wv.key_to_index) == 0:\n",
        "        raise ValueError(\"Vocabulary is empty. Check your input data or preprocessing.\")\n",
        "\n",
        "    # Train Doc2Vec models\n",
        "    print(\"Training models...\")\n",
        "    model_DM.train(train_corpus, total_examples=model_DM.corpus_count, epochs=model_DM.epochs)\n",
        "    model_DBOW.train(train_corpus, total_examples=model_DBOW.corpus_count, epochs=model_DBOW.epochs)\n",
        "\n",
        "    # Generate training data\n",
        "    print(\"Generating training data...\")\n",
        "    X_train = [(list(model_DM.dv[i]) + list(model_DBOW.dv[i])) for i in range(len(train_corpus))]\n",
        "    Y_train = [doc[1] for doc in train_data]\n",
        "\n",
        "    print(\"Generating testing data...\")\n",
        "    X_test = [(list(model_DM.infer_vector(test_corpus[i])) + list(model_DBOW.infer_vector(test_corpus[i]))) for i in range(len(test_corpus))]\n",
        "    Y_test = [doc[1] for doc in test_data]\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "def _read_corpus(data, tokens_only=False):\n",
        "    '''Helper function to prepare data for Doc2Vec.'''\n",
        "    for i, line in enumerate(data):\n",
        "        description = str(line[0]).strip()  # Ensure the description is a string and trimmed\n",
        "        if not description:\n",
        "            continue\n",
        "        tokens = gensim.utils.simple_preprocess(description)\n",
        "        if tokens_only:\n",
        "            yield tokens\n",
        "        else:\n",
        "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "# Classifier\n",
        "class ASP():\n",
        "    def __init__(self, X_train, Y_train, X_test, Y_test):\n",
        "        self.X_train, self.Y_train = X_train, Y_train\n",
        "        self.X_test, self.Y_test = X_test, Y_test\n",
        "        self.classifier = MLPClassifier(alpha=0.7, max_iter=10000)\n",
        "\n",
        "    def fit(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n",
        "\n",
        "    def predict(self):\n",
        "        prediction = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, prediction)\n",
        "        prf1 = precision_recall_fscore_support(y_true=self.Y_test, y_pred=prediction, average='weighted')\n",
        "\n",
        "        print('Evaluation Metrics:')\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print('Precision | Recall | F-Score')\n",
        "        pprint(prf1)\n",
        "\n",
        "# Dataset class\n",
        "class Dataset():\n",
        "    def __init__(self, path, project_id):\n",
        "        self.path = path\n",
        "        self.project_id = project_id\n",
        "\n",
        "# Experiment class\n",
        "class Experiment():\n",
        "    def __init__(self, train_data, test_data):\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def run(self):\n",
        "        X_train, Y_train, X_test, Y_test = preprocess(self.train_data, self.test_data)\n",
        "        classifier = ASP(X_train, Y_train, X_test, Y_test)\n",
        "        classifier.fit()\n",
        "        classifier.predict()\n",
        "\n",
        "# Script to run experiment with 80/20 train-test split\n",
        "a = Dataset('/content/mozilla_bug_report_data.csv', project_id=1)  # Replace with your dataset path\n",
        "train_data = load_data(a, train=True, percent=0.8)\n",
        "test_data = load_data(a, train=False, percent=0.8)\n",
        "\n",
        "print(time.ctime(time.time()))\n",
        "start = time.time()\n",
        "\n",
        "experiment = Experiment(train_data, test_data)\n",
        "experiment.run()\n",
        "\n",
        "print(time.ctime(time.time()))\n",
        "print('TOTAL RUNTIME: ', time.time() - start, 's')\n",
        "print('')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKgBtllhXtRF",
        "outputId": "396d7650-6fc4-4699-e2db-4f09115b3e39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 15 04:54:39 2024\n",
            "Preprocessing training data...\n",
            "Number of valid training documents: 7997\n",
            "Preprocessing test data...\n",
            "Number of valid test documents: 2000\n",
            "Building vocabulary...\n",
            "Vocabulary size (DM): 8948\n",
            "Vocabulary size (DBOW): 8947\n",
            "Training models...\n",
            "Generating training data...\n",
            "Generating testing data...\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.8235\n",
            "Precision | Recall | F-Score\n",
            "(0.7158100085230732, 0.8235, 0.7643457209738342, None)\n",
            "Sun Dec 15 04:56:22 2024\n",
            "TOTAL RUNTIME:  103.02941250801086 s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}